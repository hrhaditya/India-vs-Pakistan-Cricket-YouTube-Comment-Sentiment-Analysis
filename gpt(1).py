# -*- coding: utf-8 -*-
"""gpt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LHKKxK4Q7XDEL2l0ei-l_G3OauVdX8Xg
"""

!pip install openai tqdm
import pandas as pd
import os
import re

dataPath = "/content/youtube_comments_progress(1).csv"
df = pd.read_csv(dataPath)
df = df.dropna(subset=['text'])

# Define a function to clean the text
def preprocess_comment(comment):
    # Remove URLs
    comment = re.sub(r'http\S+|www\S+|https\S+', '', comment, flags=re.MULTILINE)
    # Remove mentions and hashtags
    comment = re.sub(r'\@\w+|\#', '', comment)
    # Lowercase the text
    comment = comment.lower()
    comment = re.sub(r'[^\w\s,]', '', comment)
    # Remove special characters and digits
    comment = re.sub(r'\W', ' ', comment)
    comment = re.sub(r'\d', '', comment)
    # Remove extra spaces
    comment = re.sub(r'\s+', ' ', comment).strip()
    return comment

# Apply the preprocessing function to the comments column
df['cleaned_comments'] = df['text'].apply(preprocess_comment).copy()

# View the cleaned comments
print(df['cleaned_comments'])

import openai
import textwrap

openai.api_key = "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"

def wrap_text(text, width=100):
    return textwrap.fill(text, width=width)

def get_gpt_response(messages):
    response = openai.chat.completions.create(
        temperature=1.0,
        top_p=0.9,
        model="gpt-4o-mini",
        messages=messages,
        max_tokens=1200,
        timeout = 400
    )
    return response.choices[0].message.content.strip()



messages = [{"role": "system", "content": "You are ChatGPT, a large language model trained by OpenAl, based on the GPT-4 architecture. Knowledge cutoff: 2021-09 Current date: Early-2024"}]
messages.append({"role": "user", "content": "hi"})

response = get_gpt_response(messages)
wrapped_response = wrap_text(response)
print(f"Response:\n{wrapped_response}")

df = pd.read_csv("/content/youtube_comments_progress(1).csv")  # Update path if needed
df = df.dropna(subset=['text']).reset_index(drop=True)
df['gpt_sentiment'] = ""

def classify_sentiment_gpt(comment):
    prompt = f"""Classify the sentiment of this YouTube comment about an India-Pakistan cricket match.
    Only respond with one word: Positive, Neutral, or Negative.

    Comment: "{comment}"
    Sentiment:"""

    try:
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=10
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print("Error:", e)
        return "Error"

from tqdm import tqdm
import openai

# Load the progress file instead of starting fresh
df = pd.read_csv("/content/checkpoint_20250427_122904.csv")  # <- adjust path as needed

# Ensure missing sentiment is represented properly
df['gpt_sentiment'] = df['gpt_sentiment'].fillna("")

# Define your GPT function again
def classify_sentiment_gpt(comment):
    prompt = f"""Classify the sentiment of this YouTube comment about an India-Pakistan cricket match.
    Only respond with one word: Positive, Neutral, or Negative.

    Comment: "{comment}"
    Sentiment:"""

    try:
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=10
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print("Error:", e)
        return "Error"

# Resume loop from where we left off
for i in tqdm(df.index):
    if df.at[i, 'gpt_sentiment'] == "":
        df.at[i, 'gpt_sentiment'] = classify_sentiment_gpt(df.at[i, 'text'])

    if i % 100 == 0:
        df.to_csv("gpt_sentiment_progress_resumed.csv", index=False)

df.to_csv("gpt_sentiment_final_resumed.csv", index=False)
print("âœ… Resumed and Done!")

from tqdm import tqdm
import openai
import pandas as pd
import time
from datetime import datetime

# Load your progress file
df = pd.read_csv("/content/checkpoint_20250427_225710.csv")
df['gpt_sentiment'] = df['gpt_sentiment'].fillna("")

# GPT-4o-mini Sentiment Classifier
def classify_sentiment_gpt(comment):
    prompt = f"""Classify the sentiment of this YouTube comment about an India-Pakistan cricket match.
    Only respond with one word: Positive, Neutral, or Negative.

    Comment: "{comment}"
    Sentiment:"""
    try:
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=10
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error at comment: {comment[:50]} - {e}")
        return "Error"

# Save function with timestamped backup
def save_checkpoint(dataframe, base_filename="gpt_sentiment_progress_resumed.csv"):
    dataframe.to_csv(base_filename, index=False)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_filename = f"checkpoint_{timestamp}.csv"
    dataframe.to_csv(backup_filename, index=False)
    print(f"ðŸ’¾ Progress saved at row {i} â€” {backup_filename}")

# Main loop with autosave
for i in tqdm(df.index):
    if df.at[i, 'gpt_sentiment'] == "":
        df.at[i, 'gpt_sentiment'] = classify_sentiment_gpt(df.at[i, 'text'])

        # Save every 50 rows
        if i % 50 == 0:
            df.to_csv("gpt_sentiment_progress_resumed.csv", index=False)

        # Also backup every 500 rows
        if i % 500 == 0:
            save_checkpoint(df)

# Final save
df.to_csv("gpt_sentiment_final_resumed.csv", index=False)
print("âœ… Resumed and Fully Done!")

