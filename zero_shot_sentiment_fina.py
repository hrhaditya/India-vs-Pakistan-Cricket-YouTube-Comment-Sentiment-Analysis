# -*- coding: utf-8 -*-
"""Untitled55.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u6EmnJaPBV0gM9f1ZVY4ziWUL0K6aIzk
"""

import pandas as pd
import os
import re

dataPath = "/content/checkpoint_zero_shot_20250428_021155.csv"
df = pd.read_csv(dataPath)
df = df.dropna(subset=['text'])

# Define a function to clean the text
def preprocess_comment(comment):
    # Remove URLs
    comment = re.sub(r'http\S+|www\S+|https\S+', '', comment, flags=re.MULTILINE)
    # Remove mentions and hashtags
    comment = re.sub(r'\@\w+|\#', '', comment)
    # Lowercase the text
    comment = comment.lower()
    comment = re.sub(r'[^\w\s,]', '', comment)
    # Remove special characters and digits
    comment = re.sub(r'\W', ' ', comment)
    comment = re.sub(r'\d', '', comment)
    # Remove extra spaces
    comment = re.sub(r'\s+', ' ', comment).strip()
    return comment

# Apply the preprocessing function to the comments column
df['cleaned_comments'] = df['text'].apply(preprocess_comment).copy()

# View the cleaned comments
print(df['cleaned_comments'])

# Step 1: Install dependencies
!pip install transformers -q

# Step 2: Import Libraries
import pandas as pd
from transformers import pipeline
from tqdm import tqdm
from datetime import datetime
import os

# Step 3: Paths & Config
progress_path = "zero_shot_sentiment_progress.csv"
initial_data_path = "/content/checkpoint_zero_shot_20250427_140853.csv"
final_output_path = "zero_shot_sentiment_final.csv"
candidate_labels = ["Positive", "Neutral", "Negative"]

# Step 1: Install necessary package
!pip install transformers -q

# Step 2: Import Libraries
import pandas as pd
from transformers import pipeline
from tqdm import tqdm
from datetime import datetime
import os

# Step 3: Paths & Labels
checkpoint_path = "/content/checkpoint_zero_shot_20250428_021155.csv"
progress_path = "zero_shot_sentiment_progress_resume.csv"
final_output_path = "zero_shot_sentiment_final_resume.csv"
candidate_labels = ["Positive", "Neutral", "Negative"]

# Step 4: Load Checkpoint
df = pd.read_csv(checkpoint_path)
df = df.dropna(subset=["text"])
if "muril_sentiment_label" not in df.columns:
    df["muril_sentiment_label"] = ""

print("✅ Loaded checkpoint with", len(df), "rows.")

# Step 5: Load Model Pipeline
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli", device=0)

# Step 6: Filter Rows to Continue Processing
mask_unlabeled = df['muril_sentiment_label'].isin(["", "Unknown"]) | df['muril_sentiment_label'].isna()
unlabeled_indices = df[mask_unlabeled].index

print("🔍 Unlabeled rows remaining:", len(unlabeled_indices))

# Step 7: Process Remaining Comments
for i in tqdm(unlabeled_indices, total=len(unlabeled_indices)):
    try:
        result = classifier(df.at[i, 'text'], candidate_labels)
        df.at[i, 'muril_sentiment_label'] = result['labels'][0]
    except Exception as e:
        print(f"⚠️ Error at row {i}: {e}")
        df.at[i, 'muril_sentiment_label'] = "Unknown"

    if i % 100 == 0:
        df.to_csv(progress_path, index=False)

    if i % 500 == 0:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = f"checkpoint_zero_shot_{timestamp}.csv"
        df.to_csv(backup_path, index=False)
        print(f"💾 Backup saved at row {i} -> {backup_path}")

# Step 8: Final Save
df.to_csv(final_output_path, index=False)
print("✅ Processing complete. Final results saved to:", final_output_path)